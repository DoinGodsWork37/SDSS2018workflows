---
title: "Data Science Workflows"
author: "Jim Harner"
date: "5/6/18"
output:
  slidy_presentation: default
---

## The Data Science Process

The data science workflow or process starts with data extraction and end with a data produce. A common version of this process is illustrated by:   

![The Data Science Process](workflow-figures/dsciProcess.png)  

This diagram is taken from [Doing Data Science](http://shop.oreilly.com/product/0636920028529.do). Notice the flow along the top where data is collected, processed, cleaned, and explored. Many conflate data science with the box on the right, i.e., machine learning algorithms and statistical models, but it is much more that that. Ultimately, we want to make decisions or to create data products, an iterative process.

## Operational Components

- R/RStudio

- Hadoop: an open-source framework for large-scale data storage and distributed computing, built on the MapReduce model.

- Spark

## Batch Architecture

![](workflow-figures/BatchArch.png) 

See O'Reilly's [Fast Data Architectures for Streaming Applications](http://www.oreilly.com/data/free/fast-data-architectures-for-streaming-applications.csp)

## Streaming Architecture

![](workflow-figures/StreamArch.png) 


## Hadley's Tidyverse

Hadley Wickham has a more abbreviated and specific version given here: [R for Data Science](http://r4ds.had.co.nz/introduction.html#what-you-will-learn).

It focuses on:

- `tidyr` to make data tidy,
- `dplyr` for transforming the data,
- `ggplot2` for visualization, and others.

A Dockerized versions of R called <[rocker](http://github.com/rocker-org/rocker)> is available.  The Docker image `verse` adds the `rstudio`, `tidyverse`, and `verse` layers to the `r-ver` base image. The tidyverse can be spun up with the following bash command:
```
sudo docker run -p 8787:8787 rocker/verse
```

## Rspark

`rspark` extends `rocker` to the Spark ecosystem. The GitHub repo for `rspark` can be found [here](http://github.com/jharner/rspark). `rspark` consists of containers for RStudio, PostgreSQL, Hadoop, Hive, and Spark. By default, Spark has a master and two workers although it can also be run within the RStudio container as an option.



